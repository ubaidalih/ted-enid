{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a72d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "import sacrebleu\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dac092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_SEQ_LEN = 64      # Maximum sequence length\n",
    "D_MODEL = 64          # Embedding dimension\n",
    "HEADS = 4             # Number of attention heads\n",
    "N_LAYERS = 2          # Number of encoder/decoder layers\n",
    "D_FF = 256            # Feed-forward dimension\n",
    "DROPOUT = 0.1         # Droput rate\n",
    "BATCH_SIZE = 64       # Batch size\n",
    "VOCAB_SIZE = 8000     # SentencePiece vocab size\n",
    "SP_MODEL_PREFIX = 'spm'\n",
    "PAD_IDX = 0           # Pad index\n",
    "BEAM_WIDTH = 4        # Beam size for beam search\n",
    "LENGTH_PENALTY = 0.6  # Length penalty for beam search\n",
    "SMOOTHING_EPS = 0.1   # Label smmothing epsilon\n",
    "WARMUP = 4000         # Learning rate scheduler warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcde736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train SentencePiece\n",
    "def train_sentencepiece(texts, model_prefix=SP_MODEL_PREFIX, vocab_size=VOCAB_SIZE):\n",
    "    \n",
    "    input_file = f\"{model_prefix}_input.txt\"\n",
    "    # write all sentences\n",
    "    with open(input_file, 'w', encoding='utf-8') as f:\n",
    "        for line in texts:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "    # train BPE model with designated special IDs\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input={input_file} --model_prefix={model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size} --character_coverage=1.0 --model_type=bpe \"\n",
    "        \"--pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\"\n",
    "        \"--minloglevel=2\"\n",
    "    )\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(f\"{model_prefix}.model\")\n",
    "    # cleanup\n",
    "    os.remove(input_file)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c379785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode with SP, adding BOS/EOS and padding\n",
    "def encode_sp(text, sp, max_len=MAX_SEQ_LEN):\n",
    "    ids = sp.EncodeAsIds(text)\n",
    "    # BOS + tokens + EOS, trimmed\n",
    "    seq = [sp.bos_id()] + ids[: max_len - 2] + [sp.eos_id()]\n",
    "    # padding\n",
    "    seq += [sp.pad_id()] * (max_len - len(seq))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):  # fixed (sinusoidal) positional encodings\n",
    "    def __init__(self, d_model, dropout=DROPOUT, max_len=MAX_SEQ_LEN):\n",
    "        super().__init__()  # initialize base class\n",
    "        pe = torch.zeros(max_len, d_model)  # (L, D) encoding table\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)  # position indices (L, 1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))  # 1 / 10000^(2i/d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * div)  # apply sine to even dimensions\n",
    "        pe[:, 1::2] = torch.cos(pos * div)  # apply cosine to odd dimensions\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # save as non‑trainable buffer (1, L, D)\n",
    "        self.dropout = nn.Dropout(dropout)  # dropout for regularization\n",
    "    def forward(self, x):  # x: (N, L, D)\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])  # add encodings & apply dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b855ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):  # core attention mechanism\n",
    "    def __init__(self, dropout=DROPOUT):\n",
    "        super().__init__(); self.dropout = nn.Dropout(dropout)  # keep‑prob for attention map\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = q.size(-1)  # dimension of key/query\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(d_k)  # scaled dot‑product (N, h, L_q, L_k)\n",
    "        if mask is not None:  # apply padding/causal mask\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))  # masked positions -> −∞\n",
    "        attn = F.softmax(scores, dim=-1)  # normalize to probabilities\n",
    "        attn = self.dropout(attn)  # dropout on weights\n",
    "        return attn @ v, attn  # weighted sum of values + attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):  # multiple parallel scaled‑dot attention heads\n",
    "    def __init__(self, heads, d_model, dropout=DROPOUT):\n",
    "        super().__init__(); assert d_model % heads == 0  # ensure divisible\n",
    "        self.d_k = d_model // heads  # per‑head dimensionality\n",
    "        self.h = heads  # number of heads\n",
    "        self.q_lin = nn.Linear(d_model, d_model)  # linear projection for queries\n",
    "        self.k_lin = nn.Linear(d_model, d_model)  # projection for keys\n",
    "        self.v_lin = nn.Linear(d_model, d_model)  # projection for values\n",
    "        self.attn = ScaledDotProductAttention(dropout)  # shared attention module\n",
    "        self.out = nn.Linear(d_model, d_model)  # final output projection\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)  # batch size\n",
    "        # shape into (N, h, L, d_k)\n",
    "        q = self.q_lin(q).view(bs, -1, self.h, self.d_k).transpose(1, 2)  # project + reshape queries\n",
    "        k = self.k_lin(k).view(bs, -1, self.h, self.d_k).transpose(1, 2)  # keys\n",
    "        v = self.v_lin(v).view(bs, -1, self.h, self.d_k).transpose(1, 2)  # values\n",
    "        x, _ = self.attn(q, k, v, mask)  # scaled‑dot attention on each head\n",
    "        x = x.transpose(1, 2).contiguous().view(bs, -1, self.h * self.d_k)  # concat heads -> (N, L, D)\n",
    "        return self.out(x)  # project back to model dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):  # position‑wise feed‑forward network\n",
    "    def __init__(self, d_model, d_ff, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(d_model, d_ff)  # expand dimensionality\n",
    "        self.lin2 = nn.Linear(d_ff, d_model)  # project back\n",
    "        self.drop = nn.Dropout(dropout)  # dropout between layers\n",
    "    def forward(self, x): \n",
    "        return self.lin2(self.drop(F.relu(self.lin1(x))))  # ReLU activation + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b335e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):  # single encoder block\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(heads, d_model, dropout)  # self‑attention sub‑layer\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)  # position‑wise FFN\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # first LayerNorm (pre‑norm)\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # second LayerNorm\n",
    "        self.drop1 = nn.Dropout(dropout)  # dropout after attention\n",
    "        self.drop2 = nn.Dropout(dropout)  # dropout after FFN\n",
    "    def forward(self, x, mask):\n",
    "        _x = self.norm1(x)  # normalize before attention\n",
    "        x = x + self.drop1(self.self_attn(_x, _x, _x, mask))  # residual self‑attention\n",
    "        _x = self.norm2(x)  # normalize before FFN\n",
    "        return x + self.drop2(self.ff(_x))  # residual feed‑forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37131ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):  # single decoder block\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(heads, d_model, dropout)  # masked self‑attention\n",
    "        self.cross_attn = MultiHeadAttention(heads, d_model, dropout)  # encoder‑decoder attention\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)  # position‑wise FFN\n",
    "        # LayerNorms\n",
    "        self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model); self.norm3 = nn.LayerNorm(d_model)\n",
    "        # Dropouts\n",
    "        self.drop1 = nn.Dropout(dropout); self.drop2 = nn.Dropout(dropout); self.drop3 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mem, tgt_mask, mem_mask):\n",
    "        _x = self.norm1(x)  # pre‑norm\n",
    "        x = x + self.drop1(self.self_attn(_x, _x, _x, tgt_mask))  # add masked self‑attention\n",
    "        _x = self.norm2(x)  # pre‑norm for cross‑attention\n",
    "        x = x + self.drop2(self.cross_attn(_x, mem, mem, mem_mask))  # add cross‑attention\n",
    "        _x = self.norm3(x)  # pre‑norm for FFN\n",
    "        return x + self.drop3(self.ff(_x))  # add feed‑forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b301744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  # encoder stack\n",
    "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)  # token embeddings\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout)  # add positional info\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])  # stacked layers\n",
    "        self.norm = nn.LayerNorm(d_model)  # final normalization\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src) * math.sqrt(self.embed.embedding_dim)  # scale embeddings by sqrt(d_model)\n",
    "        x = self.pos_enc(x)  # add positional encodings\n",
    "        for layer in self.layers:  # pass through layers\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)  # return encoded memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ea805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):  # decoder stack\n",
    "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)  # token embeddings\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout)  # positional encodings\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])  # stacked layers\n",
    "        self.norm = nn.LayerNorm(d_model)  # final normalization\n",
    "    def forward(self, tgt, mem, tgt_mask, mem_mask):\n",
    "        x = self.embed(tgt) * math.sqrt(self.embed.embedding_dim)  # scale embeddings\n",
    "        x = self.pos_enc(x)  # add positional info\n",
    "        for layer in self.layers:  # pass through decoder layers\n",
    "            x = layer(x, mem, tgt_mask, mem_mask)\n",
    "        return self.norm(x)  # return decoded representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e68ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):  # full encoder‑decoder model\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=D_MODEL, n_layers=N_LAYERS, heads=HEADS, d_ff=D_FF, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, d_model, n_layers, heads, d_ff, dropout)  # encoder stack\n",
    "        self.decoder = Decoder(vocab_size, d_model, n_layers, heads, d_ff, dropout)  # decoder stack\n",
    "        self.out = nn.Linear(d_model, vocab_size)  # tied output projection\n",
    "        self.out.weight = self.decoder.embed.weight  # weight tying with decoder embeddings\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, mem_mask):\n",
    "        mem = self.encoder(src, src_mask)  # encode source sequence\n",
    "        dec = self.decoder(tgt, mem, tgt_mask, mem_mask)  # decode target sequence\n",
    "        return self.out(dec)  # project to vocabulary logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, tgt, pad_idx=PAD_IDX):\n",
    "    # src: (B, S), tgt: (B, T)\n",
    "    # 1) Encoder padding mask: (B,1,1,S)\n",
    "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    # 2) Decoder padding mask: (B,1,1,T)\n",
    "    tgt_pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    # 3) Subsequent mask: (1,1,T,T)\n",
    "    T = tgt.size(1)\n",
    "    subsequent = torch.triu(torch.ones((T, T), device=tgt.device), diagonal=1).bool()\n",
    "    subsequent_mask = subsequent.unsqueeze(0).unsqueeze(1)\n",
    "    # 4) Combine: now (B,1,T,T)\n",
    "    tgt_mask = tgt_pad_mask & ~subsequent_mask\n",
    "    # 5) Cross-attention (same as encoder mask): (B,1,1,S)\n",
    "    memory_mask = src_mask\n",
    "    return src_mask, tgt_mask, memory_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72bf121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, tgt): self.src, self.tgt = src, tgt\n",
    "    def __len__(self): return len(self.src)\n",
    "    def __getitem__(self, i): return self.src[i], self.tgt[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src, src_mask, sp, device, beam_width=BEAM_WIDTH, max_len=MAX_SEQ_LEN, alpha=LENGTH_PENALTY):\n",
    "    model.eval()\n",
    "    # 1) Encode the source once (keys/values reused every step)\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    # 2) Each beam is (token_list, log_prob); start with BOS\n",
    "    beams      = [([sp.bos_id()], 0.0)]\n",
    "    completed  = []                           # finished beams\n",
    "    # 3) Unroll up to max_len tokens\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        # Expand every current beam\n",
    "        for tokens, score in beams:\n",
    "            # ­­­ Stop expansion if EOS already generated\n",
    "            if tokens[-1] == sp.eos_id():\n",
    "                completed.append((tokens, score))\n",
    "                continue\n",
    "            # Prepare decoder input tensor (1 × t)\n",
    "            ys = torch.tensor([tokens], device=device)\n",
    "            # Build masks (padding + subsequent)\n",
    "            tgt_pad = (ys != sp.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "            T       = ys.size(1)\n",
    "            sub     = torch.triu(torch.ones((T, T), device=device), 1).bool()\n",
    "            sub     = sub.unsqueeze(0)  # (1 × T × T)\n",
    "            # Single-step decoder produces next-token logits\n",
    "            out = model.decoder(ys, memory, tgt_pad & ~sub, src_mask)\n",
    "            log_probs = F.log_softmax(out[:, -1, :], dim=-1).squeeze(0)\n",
    "            # Pick top-k candidates and extend the beam\n",
    "            topk_logp, topk_idx = torch.topk(log_probs, beam_width)\n",
    "            for logp, idx in zip(topk_logp.tolist(), topk_idx.tolist()):\n",
    "                # Skip <pad> to avoid degenerate beams\n",
    "                if idx == sp.pad_id():\n",
    "                    continue\n",
    "                new_beams.append((tokens + [idx], score + logp))\n",
    "        # 4) Keep best `beam_width` beams using length penalty\n",
    "        beams = sorted(new_beams, key=lambda x: x[1] / (len(x[0]) ** alpha), reverse=True)[:beam_width]\n",
    "        # If no beams remain (all ended), stop early\n",
    "        if not beams:\n",
    "            break\n",
    "    # 5) Include any beams that ended with EOS\n",
    "    beams.extend(completed)\n",
    "    # 6) Choose best overall (highest length-normalised score)\n",
    "    best = max(beams, key=lambda x: x[1] / (len(x[0]) ** alpha))\n",
    "    # 7) Convert ID list → string, stripping BOS/EOS\n",
    "    return sp.DecodeIds(best[0][1:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda87ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "def get_label_smoothing_loss(vocab_size, ignore_index, eps=SMOOTHING_EPS):\n",
    "    class LabelSmoothingLoss(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conf      = 1.0 - eps  # prob. for gold token\n",
    "            self.smooth    = eps        # mass to distribute\n",
    "            self.vocab_size = vocab_size\n",
    "            self.ignore     = ignore_index\n",
    "\n",
    "        def forward(self, pred, target):\n",
    "            # pred: (N, V) un-normalised logits\n",
    "            # target: (N,) ground-truth indices\n",
    "            with torch.no_grad():\n",
    "                # 1) Start with uniform distribution\n",
    "                true_dist = torch.full_like(pred, self.smooth / (self.vocab_size - 1))\n",
    "                # 2) Put (1-eps) on the gold label\n",
    "                true_dist.scatter_(1, target.unsqueeze(1), self.conf)\n",
    "                # 3) Zero-out ignored positions (<pad>)\n",
    "                mask = target == self.ignore\n",
    "                true_dist[mask] = 0\n",
    "            # 4) KL-divergence between smoothed target and log-softmax\n",
    "            return torch.mean(torch.sum(-true_dist * F.log_softmax(pred, dim=1), dim=1))\n",
    "    return LabelSmoothingLoss()\n",
    "\n",
    "# Noam schedule\n",
    "def noam_schedule(step, d_model=D_MODEL, warmup=WARMUP):\n",
    "    step = step + 1\n",
    "    return (d_model ** -0.5) * min(step ** -0.5, step * warmup ** -1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(model, data_iter, sp, device):\n",
    "    \"\"\"\n",
    "    Evaluate corpus BLEU over a DataLoader or iterable of (src_ids, tgt_ids) pairs with progress bar.\n",
    "    \"\"\"\n",
    "    refs, hyps = [], []   # lists for sacreBLEU\n",
    "    # 1) Iterate over batches with a progress bar\n",
    "    for batch in tqdm(data_iter, desc=\"Evaluating BLEU\", unit=\"batch\"):\n",
    "        src_batch, tgt_batch = batch  # unpack tensors (B × L)\n",
    "        # 2) Decode sentence-by-sentence without gradient tracking\n",
    "        with torch.no_grad():\n",
    "            for src_ids, tgt_ids in zip(src_batch, tgt_batch):\n",
    "                # Add batch dimension and move to device\n",
    "                src = src_ids.unsqueeze(0).to(device)\n",
    "                # Source mask (padding positions = False)\n",
    "                src_mask = (src != sp.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "                # Beam-search translation hypothesis (string)\n",
    "                hyp = beam_search(model, src, src_mask, sp, device)\n",
    "                # Strip special tokens from reference\n",
    "                tgt_seq = [i for i in tgt_ids.tolist() if i not in (sp.pad_id(), sp.bos_id(), sp.eos_id())]\n",
    "                ref = sp.DecodeIds(tgt_seq)\n",
    "                # Tiny detokenisation clean-ups\n",
    "                hyp = hyp.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "                ref = ref.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "                # Accumulate\n",
    "                hyps.append(hyp)\n",
    "                refs.append(ref)\n",
    "\n",
    "    # 3) Quick sanity check – print first 5 translations\n",
    "    for i in range(5):\n",
    "        print(f\"Prediction:   {hyps[i]}\")\n",
    "        print(f\"Ground Truth: {refs[i]}\")\n",
    "\n",
    "    # 4) Corpus BLEU via sacreBLEU\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
    "    print(f\"BLEU score: {bleu.score:.2f}\")\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size=BATCH_SIZE, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"\n",
    "    Load HF dataset, train SentencePiece, and return DataLoader + SP processor + vocab info.\n",
    "    \"\"\"\n",
    "    # 1) Pull the English↔Indonesian TED dataset from Hugging Face Hub\n",
    "    data = load_dataset(\"SEACrowd/ted_en_id\", trust_remote_code=True)\n",
    "    # 2) Split handles (DatasetDict keys: 'train', 'test')\n",
    "    data_train = data['train']\n",
    "    data_test  = data['test']\n",
    "    # 3) Gather all sentences (src + tgt) to learn a joint sub-word vocab\n",
    "    all_texts = [ex['text']  for ex in data_train] + [ex['label'] for ex in data_train]\n",
    "    # 4) Train SentencePiece tokenizer → returns a processor object\n",
    "    sp = train_sentencepiece(all_texts, vocab_size=vocab_size)\n",
    "    # 5) True vocab size (may differ slightly from requested size)\n",
    "    vocab_size = sp.GetPieceSize()\n",
    "    # 6) Index of the <pad> token for later masking\n",
    "    pad_idx = sp.pad_id()\n",
    "    # 7) Encode **training** split into integer tensors\n",
    "    src_list = [torch.tensor(encode_sp(ex['text'],  sp), dtype=torch.long) for ex in data_train]\n",
    "    tgt_list = [torch.tensor(encode_sp(ex['label'], sp), dtype=torch.long) for ex in data_train]\n",
    "    # 8) Wrap as PyTorch DataLoader (shuffle = True)\n",
    "    train_loader = DataLoader(TranslationDataset(src_list, tgt_list), batch_size=batch_size, shuffle=True)\n",
    "    # 9) Encode **test** split (no shuffling)\n",
    "    src_list = [torch.tensor(encode_sp(ex['text'],  sp), dtype=torch.long) for ex in data_test]\n",
    "    tgt_list = [torch.tensor(encode_sp(ex['label'], sp), dtype=torch.long) for ex in data_test]\n",
    "    # 10) Test DataLoader (shuffle = False)\n",
    "    test_loader = DataLoader(TranslationDataset(src_list, tgt_list), batch_size=batch_size, shuffle=False)\n",
    "    # 11) Return everything needed downstream\n",
    "    return train_loader, test_loader, sp, vocab_size, pad_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46587ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, sp, vocab_size, pad_idx, device,\n",
    "                epochs=100):\n",
    "    \"\"\"\n",
    "    Train the Transformer model with label smoothing and Noam schedule.\n",
    "    \"\"\"\n",
    "    # 1) Criterion: custom label-smoothing cross-entropy\n",
    "    criterion = get_label_smoothing_loss(vocab_size, pad_idx, eps=SMOOTHING_EPS)\n",
    "    # 2) Adam (β₁=0.9, β₂=0.98) with a dummy LR—Noam will rescale it\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    # 3) Noam LR schedule: warm-up then 1/√step decay\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: noam_schedule(step, warmup=WARMUP))\n",
    "\n",
    "    # 4) Epoch loop with tqdm progress bar\n",
    "    pbar = trange(1, epochs + 1, desc=\"Training\", unit=\"epoch\")\n",
    "    for epoch in pbar:\n",
    "        model.train()            # switch to training mode\n",
    "        total_loss = 0.0         # accumulate loss per epoch\n",
    "        # 5) Mini-batch loop\n",
    "        for step, (src_batch, tgt_batch) in enumerate(train_loader, start=1):\n",
    "            # Move to GPU / CPU\n",
    "            src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "            # Decoder input is everything except the last token\n",
    "            tgt_input = tgt_batch[:, :-1]\n",
    "            # Build padding + subsequent-mask matrices\n",
    "            s_mask, t_mask, m_mask = create_masks(src_batch, tgt_input, pad_idx)\n",
    "            # Forward pass (logits shape: B × T × V)\n",
    "            logits = model(src_batch, tgt_input, s_mask, t_mask, m_mask)\n",
    "            # Loss: compare logits against ground-truth next-tokens\n",
    "            loss = criterion(logits.view(-1, vocab_size), tgt_batch[:, 1:].reshape(-1))\n",
    "            # Standard optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        # 6) Epoch-level logging\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(loss=f\"{avg_loss:.4f}\", lr=f\"{lr:.6f}\")\n",
    "    # 7) Return fine-tuned model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e4ecc",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6452d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0e0158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      1,265,728\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82be6c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [34:55<00:00, 20.95s/epoch, loss=1.5462, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43dacb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [19:23<00:00, 23.27s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada akhir tahun ini, ada di planet ini akan ada orang-orang di planet sosial yang menggunakan jaringan sosial.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Salah satu hal yang umum di semua itu adalah mereka akan mati.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Mungkin itu mungkin menjadi moralitas, saya berpikir bahwa hal itu sangat penting.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama saya pikirkan tentang blog ini adalah sebuah blog ini oleh D. Rill, yang telah meninggal pada tahun Mill, dan ilmuwan yang meninggal.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang dilakukan Milli adalah teman-teman dan teman-teman mereka pulang setelah dia meninggal.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 13.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.820058819427818"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2a731",
   "metadata": {},
   "source": [
    "## Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea21057",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data(vocab_size=4000)\n",
    "model = Transformer(vocab_size=4000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b56e07ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      749,728\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fba95c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [35:22<00:00, 21.23s/epoch, loss=1.6680, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b85c4bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 50/50 [20:18<00:00, 24.36s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Di akhir tahun ini , ada beberapa milyar orang yang akan duduk di planet ini .\n",
      "Ground Truth: Pada akhir tahun ini , akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif .\n",
      "Prediction: Satu hal yang sama dengan mereka adalah bahwa mereka akan mati .\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal .\n",
      "Prediction: Siapa yang mungkin adalah sebuah pagi , saya pikir , saya berpikir bahwa saya pikir itu benar-benar mengeksplorasi .\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan , saya pikir ada implikasi yang mendalam yang perlu ditelusuri .\n",
      "Prediction: Apa yang pertama saya pikirkan tentang saya tentang ini adalah seorang profesor ini adalah seorang wanita yang telah meninggal oleh Kh , dan ilmu pengetahuan yang meninggal .\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller , seorang jurnalis ilmiah dan teknologi , yang meninggal karena kanker .\n",
      "Prediction: Dan apa yang dilakukan dengan teman-teman dan teman-teman itu dia pergi ke depan .\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score (beam=4): 12.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.64693430106625"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb3d84",
   "metadata": {},
   "source": [
    "## Model Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1699e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer(d_model=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34ccce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      612,800\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c2a6354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [33:52<00:00, 20.33s/epoch, loss=1.7332, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cb01928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|██████████| 50/50 [18:44<00:00, 22.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Dalam akhir tahun ini, ada di sini, ada di planet ini di planet ini di planet sosial.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Hal ini adalah satu hal yang harus mereka lakukan di sana.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Apakah hal ini mungkin berpikir bahwa saya pikir, saya pikir bahwa saya pikir bahwa hal yang sangat penting.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama saya berpikir tentang apa yang pertama ini adalah sebuah film yang telah dilakukan oleh para ilmuwan yang meninggal, yang meninggal oleh para ahli fisika, dan teknologi yang meninggal.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang saya lakukan adalah teman-teman yang saya lakukan dan menulisnya.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 9.69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.694397018993156"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a36e40",
   "metadata": {},
   "source": [
    "## Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer(n_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5c07070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      1,148,992\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78e7e2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [21:20<00:00, 12.81s/epoch, loss=1.6207, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3daee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [11:56<00:00, 14.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada akhir tahun ini, ada satu miliar orang yang akan digunakan di planet ini.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Salah satu hal yang umum adalah mereka akan meninggal di mana mereka meninggal.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Apa yang mungkin saya berpikir bahwa saya berpikir bahwa saya berpikir bahwa beberapa hal yang sangat penting.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama saya berpikir tentang blog ini adalah sebuah blog yang telah dilakukan oleh Dil, D. D. bernama D. dan teknologi yang meninggal.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang dilakukan oleh teman-temannya dan teman-temannya meninggal.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 10.39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.394259656467081"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6814e",
   "metadata": {},
   "source": [
    "## Number of Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fefc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer(heads=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3882ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      1,265,728\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd9a09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [33:46<00:00, 20.26s/epoch, loss=1.5423, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a215cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [21:17<00:00, 25.56s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada akhir tahun ini, ada satu miliar orang di planet ini yang menggunakan jaringan sosial yang menggunakan jaringan sosial.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Salah satu hal yang kita miliki di mana mereka akan mati.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Apapun yang mungkin menjadi sangat penting, saya pikir saya berpikir bahwa hal ini sangat besar.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama yang pertama saya pikirkan tentang blog ini adalah blog di Dill, Dill, seorang jurnalis, yang seorang jurnalis yang meninggal.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang saya lakukan adalah teman-teman dan menulis seorang teman-teman yang meninggal setelah dia meninggal.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 14.36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.36012738155998"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb6884",
   "metadata": {},
   "source": [
    "# Softpick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f5e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softpick(x: torch.Tensor, *, dim: int = -1, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    SoftPick:  ReLU(exp(x) - 1) / Σ|exp(x) - 1|\n",
    "    • identical interface to F.softmax\n",
    "    • numerically stable on large |x| by subtracting max(x)\n",
    "    • differentiable everywhere except at the ReLU knee (same as ReLU)\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    x   : (..., n) – raw attention scores\n",
    "    dim : axis that should sum to 1\n",
    "    eps : keeps gradients finite when the entire row is exactly zero\n",
    "    \"\"\"\n",
    "    x_m = torch.max(x, dim=dim, keepdim=True).values\n",
    "    x_m_e_m = torch.exp(-x_m)\n",
    "    x_e_1 = torch.exp(x - x_m) - x_m_e_m\n",
    "    r_x_e_1 = F.relu(x_e_1)\n",
    "    a_x_e_1 = torch.where(x.isfinite(), torch.abs(x_e_1), 0)\n",
    "    return r_x_e_1 / (torch.sum(a_x_e_1, dim=dim, keepdim=True) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22628c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=DROPOUT): super().__init__(); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = q.size(-1)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(d_k)\n",
    "        if mask is not None: scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = softpick(scores, dim=-1); attn = self.dropout(attn)\n",
    "        return attn @ v, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd568b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f022c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      1,265,728\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcde86f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [38:54<00:00, 23.34s/epoch, loss=1.5406, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7adf977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [27:39<00:00, 33.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada akhir tahun ini, ada beberapa miliar orang di planet ini di planet sosial yang menggunakan jaringan sosial.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Salah satu hal yang sama dari mereka memiliki mereka akan mati.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Apakah hal itu mungkin menjadi sebuah moralitas, saya pikir bahwa saya benar-benar memiliki dampak yang sangat penting.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama kali saya berpikir tentang blog ini adalah sebuah blog dari D. D. D. D. dan seorang jurnalis yang meninggal oleh teknologi yang meninggal.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang telah dilakukan oleh keluarga dan teman-temannya menjadi teman-teman yang meninggal setelah dia meninggal.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 13.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.974490198992573"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b344c2",
   "metadata": {},
   "source": [
    "## Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data(vocab_size=4000)\n",
    "model = Transformer(vocab_size=4000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6069bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      749,728\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22f983bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [40:51<00:00, 24.51s/epoch, loss=1.6524, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51ac28a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [28:09<00:00, 33.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada akhir tahun ini, ada sekitar 1 miliar orang di planet ini di planet ini.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Yang salah satu hal yang ada di dalam mereka adalah bahwa mereka akan meninggal.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Siapa yang mungkin ada seorang profesor, saya berpikir bahwa saya pikir ini sangat buruk.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama saya pikirkan adalah tentang otomatis ini adalah per tahun ini oleh D. D. D. D. Mart, seorang ilmuwan yang telah meninggal.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang dilakukan Mer adalah teman-teman dan teman-teman mereka menulis bahwa dia meninggal.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 12.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.915632463105004"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3901e9d",
   "metadata": {},
   "source": [
    "## Model Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer(d_model=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae545c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      612,800\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4bc687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [39:42<00:00, 23.82s/epoch, loss=1.7400, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11e9a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [26:01<00:00, 31.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada tahun ini, ada beberapa miliar orang-orang yang akan berada di planet ini di planet ini di planet ini.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Salah satu hal yang lain adalah bahwa mereka akan mati.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Apakah itu mungkin akan benar-benar benar-benar benar-benar berpikir, saya berpikir bahwa beberapa hal yang sangat besar.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama tentang saya tentang buku ini adalah sebuah buku yang baru yang telah saya, dengan teknologi yang telah bekerja dengan teknologi ini, dan pemerintah.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang Anda lakukan adalah teman-teman dan teman-teman dia pergi ke keluarga.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 9.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.496860251142657"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6cee4b",
   "metadata": {},
   "source": [
    "## Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer(n_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81e330ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      1,148,992\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9503eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [24:11<00:00, 14.51s/epoch, loss=1.6227, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef131f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [15:15<00:00, 18.31s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada tahun ini, ada satu miliar orang-orang di planet ini akan menggunakan jaringan sosial.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Yang satu hal yang sama yang akan meninggal.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Mungkin itu mungkin menjadi hal itu, saya berpikir bahwa saya berpikir bahwa saya berpikir bahwa hal yang sangat besar.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama saya pikirkan tentang blog ini adalah sebuah blog yang telah meninggal oleh seorang ilmuwan yang telah meninggal pada tahun yang telah meninggal oleh R. R. dan Mesir.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang telah dilakukan seorang teman dan temannya.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 9.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.892148829116039"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3dc42d",
   "metadata": {},
   "source": [
    "## Number of Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c646bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, test_loader, sp, vocab_size, pad_idx = prepare_data()\n",
    "model = Transformer(heads=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1f87c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:      1,265,728\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters:      {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2369362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [40:34<00:00, 24.35s/epoch, loss=1.5546, lr=0.000338]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, sp, vocab_size, pad_idx, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b742f6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [25:03<00:00, 30.06s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Pada akhir tahun ini, ada hampir 1 miliar orang akan duduk di planet ini di planet ini.\n",
      "Ground Truth: Pada akhir tahun ini, akan ada hampir satu milyar orang di planet ini yang menggunakan situs jejaring sosial secara aktif.\n",
      "Prediction: Yang salah satu hal yang ada di seluruh mereka akan mati.\n",
      "Ground Truth: Satu kesamaan dari mereka semua adalah mereka akan meninggal.\n",
      "Prediction: Apapun yang mungkin menjadi ide yang mungkin saya pikir, saya pikir bahwa hal ini sangat penting yang sangat penting.\n",
      "Ground Truth: Walau itu pemikiran yang agak menakutkan, saya pikir ada implikasi yang mendalam yang perlu ditelusuri.\n",
      "Prediction: Apa yang pertama saya pikirkan tentang blog ini adalah sebuah blog ini adalah sebuah blog ini adalah seorang jurnalis bernama KB, yang meninggal, yang meninggal oleh para jurnalis yang meninggal.\n",
      "Ground Truth: Yang membuat saya mulai berpikir seperti ini adalah sebuah tulisan blog pada awal tahun ini dari Derek K. Miller, seorang jurnalis ilmiah dan teknologi, yang meninggal karena kanker.\n",
      "Prediction: Dan apa yang dilakukan M. dan teman-temannya menulis seorang teman-teman itu meninggal setelah dia meninggal.\n",
      "Ground Truth: Miller meminta keluarga dan temannya untuk menulis sebuah tulisan yang keluar tak lama setelah dia meninggal.\n",
      "BLEU score: 13.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.516293466216013"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(trained_model, test_loader, sp, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
